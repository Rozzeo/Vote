{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtwtLj5KOVcVcWwME11P3M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rozzeo/Vote/blob/main/ai_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2-V_10opoc6"
      },
      "outputs": [],
      "source": [
        "!pip install -q huggingface_hub\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Khz6D3Sps65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "# os.environ[\"HF_TOKEN\"]=\"hf_xxxxxxxxxxx\"\n",
        "\n",
        "client = InferenceClient(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
        "# if the outputs for next cells are wrong, the free model may be overloaded. You can also use this public endpoint that contains Llama-3.2-3B-Instruct\n",
        "client = InferenceClient(\"https://jc26mwg228mkj8dw.us-east-1.aws.endpoints.huggingface.cloud\")"
      ],
      "metadata": {
        "id": "fDSN6ly4ps-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As seen in the LLM section, if we just do decoding, **the model will only stop when it predicts an EOS token**,\n",
        "# and this does not happen here because this is a conversational (chat) model and we didn't apply the chat template it expects.\n",
        "output = client.text_generation(\n",
        "    \"The capital of israel is\",\n",
        "    max_new_tokens=100,\n",
        ")\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzIYWzQLp01u",
        "outputId": "e5407f97-ab0f-41f4-870f-2384e94945b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Jerusalem, but the largest city is Tel Aviv.\n",
            "The capital of Israel is indeed Jerusalem, but the largest city is Tel Aviv. This is a common misconception, and it's great that you're looking to clarify the information.\n",
            "\n",
            "Jerusalem is considered the spiritual and cultural capital of Israel, and it's home to many important historical and religious sites, including the Western Wall and the Church of the Holy Sepulchre. However, Tel Aviv is the largest city in Israel, with a population of over\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If we now add the special tokens related to Llama3.2 model, the behaviour changes and is now the expected one.\n",
        "prompt=\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "The capital of israel is<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\"\n",
        "output = client.text_generation(\n",
        "    prompt,\n",
        "    max_new_tokens=100,\n",
        ")\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8rBsqpYqCZK",
        "outputId": "5318b531-e000-4ba9-fa19-785eceed3ba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tel Aviv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If we now add the special tokens related to Llama3.2 model, the behaviour changes and is now the expected one.\n",
        "prompt=\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "The capital of israel is<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\"\n",
        "output = client.text_generation(\n",
        "    prompt,\n",
        "    max_new_tokens=100,\n",
        ")\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaq7cz75rAqK",
        "outputId": "99de9a85-d47b-48e3-be18-2fbbfe72aac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tel Aviv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"The capital of israel is\"},\n",
        "    ],\n",
        "    stream=False,\n",
        "    max_tokens=1024,\n",
        ")\n",
        "\n",
        "print(output.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhKIxJEDqRIp",
        "outputId": "3bc9b79d-06af-4dcb-fbaf-b9632ba15fe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jerusalem.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This system prompt is a bit more complex and actually contains the function description already appended.\n",
        "# Here we suppose that the textual description of the tools has already been appended\n",
        "SYSTEM_PROMPT = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
        "\n",
        "get_weather: Get the current weather in a given location\n",
        "\n",
        "The way you use the tools is by specifying a json blob.\n",
        "Specifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here).\n",
        "\n",
        "The only values that should be in the \"action\" field are:\n",
        "get_weather: Get the current weather in a given location, args: {\"location\": {\"type\": \"string\"}}\n",
        "example use :\n",
        "```\n",
        "{{\n",
        "  \"action\": \"get_weather\",\n",
        "  \"action_input\": {\"location\": \"Beer Sheva\"}\n",
        "}}\n",
        "\n",
        "ALWAYS use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about one action to take. Only one action at a time in this format:\n",
        "Action:\n",
        "```\n",
        "$JSON_BLOB\n",
        "```\n",
        "Observation: the result of the action. This Observation is unique, complete, and the source of truth.\n",
        "... (this Thought/Action/Observation can repeat N times, you should take several steps when needed. The $JSON_BLOB must be formatted as markdown and only use a SINGLE action at a time.)\n",
        "\n",
        "You must always end your output with the following format:\n",
        "\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "Now begin! Reminder to ALWAYS use the exact characters `Final Answer:` when you provide a definitive answer. \"\"\""
      ],
      "metadata": {
        "id": "GWJ8W0w9rdvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Since we are running the \"text_generation\", we need to add the right special tokens.\n",
        "prompt=f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "{SYSTEM_PROMPT}\n",
        "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "What's the weather in Beer Sheva ?\n",
        "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_2J8eMK4rNz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asKjxDd3rzm1",
        "outputId": "24b25e0e-9d8c-4f14-dad6-c42fbcbb64bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "Answer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "get_weather: Get the current weather in a given location\n",
            "\n",
            "The way you use the tools is by specifying a json blob.\n",
            "Specifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here).\n",
            "\n",
            "The only values that should be in the \"action\" field are:\n",
            "get_weather: Get the current weather in a given location, args: {\"location\": {\"type\": \"string\"}}\n",
            "example use :\n",
            "```\n",
            "{{\n",
            "  \"action\": \"get_weather\",\n",
            "  \"action_input\": {\"location\": \"Beer Sheva\"}\n",
            "}}\n",
            "\n",
            "ALWAYS use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about one action to take. Only one action at a time in this format:\n",
            "Action:\n",
            "```\n",
            "$JSON_BLOB\n",
            "```\n",
            "Observation: the result of the action. This Observation is unique, complete, and the source of truth.\n",
            "... (this Thought/Action/Observation can repeat N times, you should take several steps when needed. The $JSON_BLOB must be formatted as markdown and only use a SINGLE action at a time.)\n",
            "\n",
            "You must always end your output with the following format:\n",
            "\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Now begin! Reminder to ALWAYS use the exact characters `Final Answer:` when you provide a definitive answer. \n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "What's the weather in Beer Sheva ?\n",
            "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Do you see the problem?\n",
        "output = client.text_generation(\n",
        "    prompt,\n",
        "    max_new_tokens=200,\n",
        ")\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdBeyQ4rr373",
        "outputId": "96750fb4-85db-4ecf-b776-6ac20cbecdc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"get_weather\",\n",
            "  \"action_input\": {\"location\": \"Beer Sheva\"}\n",
            "}\n",
            "```\n",
            "Observation: The current weather in Beer Sheva is mostly sunny with a high of 28°C and a low of 18°C.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The answer was hallucinated by the model. We need to stop to actually execute the function!\n",
        "output = client.text_generation(\n",
        "    prompt,\n",
        "    max_new_tokens=200,\n",
        "    stop=[\"Observation:\"] # Let's stop before any actual function is called\n",
        ")\n",
        "\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmzUXHH4tigr",
        "outputId": "fc8ec148-9405-48f2-e4c8-be017763d6a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"get_weather\",\n",
            "  \"action_input\": {\"location\": \"Beer Sheva\"}\n",
            "}\n",
            "```\n",
            "Observation:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy function\n",
        "def get_weather(location):\n",
        "    return f\"the weather in {location} is sunny with low temperatures. \\n\"\n",
        "\n",
        "get_weather('Beer Sheva')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "7vcMBWgFu91e",
        "outputId": "40446cd5-c423-4bb7-83b9-ce2a245a7269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the weather in Beer Sheva is sunny with low temperatures. \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's concatenate the base prompt, the completion until function execution and the result of the function as an Observation\n",
        "new_prompt=prompt+output+get_weather('Beer Sheva')\n",
        "print(new_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5zC8W4qvEKa",
        "outputId": "e5a6e08b-38a6-4134-efae-7993e729deaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "Answer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "get_weather: Get the current weather in a given location\n",
            "\n",
            "The way you use the tools is by specifying a json blob.\n",
            "Specifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here).\n",
            "\n",
            "The only values that should be in the \"action\" field are:\n",
            "get_weather: Get the current weather in a given location, args: {\"location\": {\"type\": \"string\"}}\n",
            "example use :\n",
            "```\n",
            "{{\n",
            "  \"action\": \"get_weather\",\n",
            "  \"action_input\": {\"location\": \"Beer Sheva\"}\n",
            "}}\n",
            "\n",
            "ALWAYS use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about one action to take. Only one action at a time in this format:\n",
            "Action:\n",
            "```\n",
            "$JSON_BLOB\n",
            "```\n",
            "Observation: the result of the action. This Observation is unique, complete, and the source of truth.\n",
            "... (this Thought/Action/Observation can repeat N times, you should take several steps when needed. The $JSON_BLOB must be formatted as markdown and only use a SINGLE action at a time.)\n",
            "\n",
            "You must always end your output with the following format:\n",
            "\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Now begin! Reminder to ALWAYS use the exact characters `Final Answer:` when you provide a definitive answer. \n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "What's the weather in Beer Sheva ?\n",
            "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"get_weather\",\n",
            "  \"action_input\": {\"location\": \"Beer Sheva\"}\n",
            "}\n",
            "```\n",
            "Observation:the weather in Beer Sheva is sunny with low temperatures. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_output = client.text_generation(\n",
        "    new_prompt,\n",
        "    max_new_tokens=200,\n",
        ")\n",
        "\n",
        "print(final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRe1tSduvT6J",
        "outputId": "11f69cfb-35ce-43aa-b49f-b3f6f0a946cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Answer: The weather in Beer Sheva is sunny with low temperatures.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install smolagents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tsGczLqzLNb",
        "outputId": "fde10bd3-3162-4d68-df7a-b69a058f53bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting smolagents\n",
            "  Downloading smolagents-1.11.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from smolagents) (0.28.1)\n",
            "Requirement already satisfied: requests>=2.32.3 in /usr/local/lib/python3.11/dist-packages (from smolagents) (2.32.3)\n",
            "Requirement already satisfied: rich>=13.9.4 in /usr/local/lib/python3.11/dist-packages (from smolagents) (13.9.4)\n",
            "Collecting pandas>=2.2.3 (from smolagents)\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from smolagents) (3.1.6)\n",
            "Requirement already satisfied: pillow>=11.0.0 in /usr/local/lib/python3.11/dist-packages (from smolagents) (11.1.0)\n",
            "Collecting markdownify>=0.14.1 (from smolagents)\n",
            "  Downloading markdownify-1.1.0-py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting duckduckgo-search>=6.3.7 (from smolagents)\n",
            "  Downloading duckduckgo_search-7.5.2-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting python-dotenv (from smolagents)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search>=6.3.7->smolagents) (8.1.8)\n",
            "Collecting primp>=0.14.0 (from duckduckgo-search>=6.3.7->smolagents)\n",
            "  Downloading primp-0.14.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search>=6.3.7->smolagents) (5.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.0->smolagents) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.0->smolagents) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.0->smolagents) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.0->smolagents) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.0->smolagents) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.0->smolagents) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=3.1.4->smolagents) (3.0.2)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.9 in /usr/local/lib/python3.11/dist-packages (from markdownify>=0.14.1->smolagents) (4.13.3)\n",
            "Requirement already satisfied: six<2,>=1.15 in /usr/local/lib/python3.11/dist-packages (from markdownify>=0.14.1->smolagents) (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->smolagents) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->smolagents) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->smolagents) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->smolagents) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.3->smolagents) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.3->smolagents) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.3->smolagents) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.3->smolagents) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.9.4->smolagents) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.9.4->smolagents) (2.18.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5,>=4.9->markdownify>=0.14.1->smolagents) (2.6)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->smolagents) (0.1.2)\n",
            "Downloading smolagents-1.11.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading duckduckgo_search-7.5.2-py3-none-any.whl (20 kB)\n",
            "Downloading markdownify-1.1.0-py3-none-any.whl (13 kB)\n",
            "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading primp-0.14.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-dotenv, primp, pandas, markdownify, duckduckgo-search, smolagents\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed duckduckgo-search-7.5.2 markdownify-1.1.0 pandas-2.2.3 primp-0.14.0 python-dotenv-1.0.1 smolagents-1.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from smolagents import CodeAgent,DuckDuckGoSearchTool, HfApiModel,load_tool,tool\n",
        "import datetime\n",
        "import requests\n",
        "import pytz\n",
        "import yaml\n",
        "from tools.final_answer import FinalAnswerTool\n",
        "\n",
        "from Gradio_UI import GradioUI\n",
        "\n",
        "# Below is an example of a tool that does nothing. Amaze us with your creativity !\n",
        "@tool\n",
        "def my_custom_tool(arg1:str, arg2:int)-> str: #it's import to specify the return type\n",
        "    #Keep this format for the description / args / args description but feel free to modify the tool\n",
        "    \"\"\"A tool that does nothing yet\n",
        "    Args:\n",
        "        arg1: the first argument\n",
        "        arg2: the second argument\n",
        "    \"\"\"\n",
        "    return \"What magic will you build ?\"\n",
        "\n",
        "@tool\n",
        "def get_current_time_in_timezone(timezone: str) -> str:\n",
        "    \"\"\"A tool that fetches the current local time in a specified timezone.\n",
        "    Args:\n",
        "        timezone: A string representing a valid timezone (e.g., 'America/New_York').\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create timezone object\n",
        "        tz = pytz.timezone(timezone)\n",
        "        # Get current time in that timezone\n",
        "        local_time = datetime.datetime.now(tz).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        return f\"The current local time in {timezone} is: {local_time}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching time for timezone '{timezone}': {str(e)}\"\n",
        "\n",
        "\n",
        "final_answer = FinalAnswerTool()\n",
        "\n",
        "# If the agent does not answer, the model is overloaded, please use another model or the following Hugging Face Endpoint that also contains qwen2.5 coder:\n",
        "# model_id='https://pflgm2locj2t89co.us-east-1.aws.endpoints.huggingface.cloud'\n",
        "\n",
        "model = HfApiModel(\n",
        "max_tokens=2096,\n",
        "temperature=0.5,\n",
        "model_id='Qwen/Qwen2.5-Coder-32B-Instruct',# it is possible that this model may be overloaded\n",
        "custom_role_conversions=None,\n",
        ")\n",
        "\n",
        "\n",
        "# Import tool from Hub\n",
        "image_generation_tool = load_tool(\"agents-course/text-to-image\", trust_remote_code=True)\n",
        "\n",
        "with open(\"prompts.yaml\", 'r') as stream:\n",
        "    prompt_templates = yaml.safe_load(stream)\n",
        "\n",
        "agent = CodeAgent(\n",
        "    model=model,\n",
        "    tools=[final_answer], ## add your tools here (don't remove final answer)\n",
        "    max_steps=6,\n",
        "    verbosity_level=1,\n",
        "    grammar=None,\n",
        "    planning_interval=None,\n",
        "    name=None,\n",
        "    description=None,\n",
        "    prompt_templates=prompt_templates\n",
        ")\n",
        "\n",
        "\n",
        "GradioUI(agent).launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "i_3hrJchvhHL",
        "outputId": "1b10c217-31db-4a24-8d1b-622ba43243b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'smolagents'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-eaaee72be3d3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msmolagents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCodeAgent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDuckDuckGoSearchTool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHfApiModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mload_tool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpytz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'smolagents'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f12a9G2NzIju"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}